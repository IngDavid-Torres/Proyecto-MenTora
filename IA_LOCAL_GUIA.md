# ğŸ¤– GuÃ­a de IA Local para MenTora

## ğŸ“‹ Opciones Disponibles

### 1. ğŸš€ **Plantillas (Recomendado para demostraciÃ³n)**
- âœ… **Ya funciona** - No requiere instalaciÃ³n adicional
- âœ… RÃ¡pido y confiable
- âœ… Ideal para demostrar funcionalidad
- âœ… Funciona completamente offline

### 2. ğŸ¦™ **Ollama (IA Avanzada)**
- Modelos locales de alta calidad
- Requiere instalaciÃ³n de Ollama

#### InstalaciÃ³n de Ollama:
```bash
# 1. Descargar e instalar Ollama desde https://ollama.ai
# 2. Instalar un modelo (elige uno):

# Modelo pequeÃ±o y rÃ¡pido (recomendado):
ollama pull llama3.2:1b

# Modelo mediano (mejor calidad):
ollama pull llama3.2:3b

# Modelo completo (mÃ¡xima calidad, requiere mÃ¡s RAM):
ollama pull llama3.2

# Alternativa - Mistral (muy bueno para espaÃ±ol):
ollama pull mistral
```

#### Verificar instalaciÃ³n:
```bash
# Verificar que Ollama estÃ© corriendo:
curl http://localhost:11434/api/tags
```

### 3. ğŸ¤— **Hugging Face Transformers**
- Modelos preentrenados
- Descarga automÃ¡tica la primera vez

#### InstalaciÃ³n:
```bash
pip install transformers torch
```

## ğŸ”§ ConfiguraciÃ³n en MenTora

### MÃ©todo 1: Usar Plantillas (Sin instalaciÃ³n)
1. Selecciona "ğŸš€ Plantillas (RÃ¡pido)" en el generador
2. Â¡Ya funciona! Genera preguntas inteligentes basadas en plantillas

### MÃ©todo 2: Configurar Ollama
1. Instala Ollama y un modelo (ver arriba)
2. AsegÃºrate de que Ollama estÃ© corriendo
3. Selecciona "ğŸ¦™ Ollama" en el generador
4. Si falla, automÃ¡ticamente usa plantillas como respaldo

### MÃ©todo 3: Configurar Hugging Face
1. Instala las dependencias: `pip install transformers torch`
2. Selecciona "ğŸ¤— Hugging Face" en el generador
3. La primera vez descargarÃ¡ el modelo automÃ¡ticamente

## ğŸ¯ Ejemplos de Uso

### MatemÃ¡ticas:
```
Tema: "Ã¡lgebra bÃ¡sica"
Resultado: "Â¿CuÃ¡l es el resultado de 15 + 23?"
Opciones: [38, 35, 40, 42]
Respuesta: A
```

### Ciencias:
```
Tema: "biologÃ­a"  
Resultado: "Â¿CuÃ¡l es la funciÃ³n principal del corazÃ³n en el cuerpo humano?"
```

### Historia:
```
Tema: "independencia de mÃ©xico"
Resultado: "Â¿En quÃ© aÃ±o ocurriÃ³ la Independencia de MÃ©xico?"
```

## ğŸš¨ SoluciÃ³n de Problemas

### Si Ollama no funciona:
1. Verifica que estÃ© instalado: `ollama --version`
2. Verifica que estÃ© corriendo: `ollama serve`
3. Prueba descargar un modelo: `ollama pull llama3.2:1b`

### Si Hugging Face da error:
1. Instala dependencias: `pip install transformers torch`
2. Verifica espacio en disco (los modelos pueden ser grandes)
3. En la primera ejecuciÃ³n puede tardar en descargar

### Si todo falla:
- **Â¡No hay problema!** El sistema siempre usa plantillas como respaldo
- Las plantillas generan preguntas inteligentes sin necesidad de IA externa

## ğŸ’¡ Recomendaciones

### Para DemostraciÃ³n:
- **Usa Plantillas**: RÃ¡pido, confiable, ya funciona

### Para ProducciÃ³n:
- **Usa Ollama**: Mejor calidad, completamente local, sin costos

### Para Desarrollo:
- **Usa Hugging Face**: Buena calidad, fÃ¡cil de configurar

## ğŸ” Verificar que Todo Funciona

```bash
# Ejecutar el test del generador:
cd /ruta/a/MenTora
python ai_local.py
```

DeberÃ­as ver preguntas generadas exitosamente.

## ğŸ“Š ComparaciÃ³n de MÃ©todos

| MÃ©todo | Velocidad | Calidad | InstalaciÃ³n | TamaÃ±o |
|--------|-----------|---------|-------------|---------|
| Plantillas | â­â­â­â­â­ | â­â­â­ | âœ… Ya funciona | 0 MB |
| Ollama | â­â­â­ | â­â­â­â­â­ | Manual | 500MB - 4GB |
| Hugging Face | â­â­ | â­â­â­â­ | AutomÃ¡tica | 200MB - 1GB |

## ğŸ“ Para Profesores

La implementaciÃ³n de IA local en MenTora demuestra:

1. **Independencia tecnolÃ³gica**: No dependemos de servicios externos
2. **Privacidad**: Los datos nunca salen del servidor local
3. **Costos**: Sin gastos en APIs externas
4. **Flexibilidad**: MÃºltiples mÃ©todos segÃºn las necesidades
5. **Robustez**: Sistema de respaldo automÃ¡tico

Â¡Perfecto para mostrar el uso prÃ¡ctico de IA en educaciÃ³n!